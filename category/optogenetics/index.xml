<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Optogenetics | Open Neuroscience</title><link>https://open-neuroscience.com/category/optogenetics/</link><atom:link href="https://open-neuroscience.com/category/optogenetics/index.xml" rel="self" type="application/rss+xml"/><description>Optogenetics</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>CC BY SA 4.0</copyright><lastBuildDate>Tue, 23 Feb 2021 00:00:00 +0000</lastBuildDate><image><url>https://open-neuroscience.com/media/openneuroscience_logo_dark.svg</url><title>Optogenetics</title><link>https://open-neuroscience.com/category/optogenetics/</link></image><item><title>Efficient training of mice on the 5-choice serial reaction time task in an automated rodent training system</title><link>https://open-neuroscience.com/post/efficient_training_of_mice_on_the_5_choice_serial_reaction_time_task_in_an_automated_rodent_training_system/</link><pubDate>Tue, 23 Feb 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/efficient_training_of_mice_on_the_5_choice_serial_reaction_time_task_in_an_automated_rodent_training_system/</guid><description>&lt;p>Experiments aiming to understand sensory-motor systems, cognition and behavior necessitate training animals to perform complex tasks. Traditional training protocols require lab personnel to move the animals between home cages and training chambers, to start and end training sessions, and in some cases, to hand-control each training trial. Human labor not only limits the amount of training per day, but also introduces several sources of variability and may increase animal stress. Here we present an automated training system for the 5-choice serial reaction time task (5CSRTT), a classic rodent task often used to test sensory detection, sustained attention and impulsivity. We found that full automation without human intervention allowed rapid, cost-efficient training, and decreased stress as measured by corticosterone levels. Training breaks introduced only a transient drop in performance, and mice readily generalized across training systems when transferred from automated to manual protocols. We further validated our automated training system with wireless optogenetics and pharmacology experiments, expanding the breadth of experimental needs our system may fulfill. Our automated 5CSRTT system can serve as a prototype for fully automated behavioral training, with methods and principles transferrable to a range of rodent tasks.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Eszter Birtalan; Anita Bánhidi; Joshua I Sanders; Diána Balázsfi; Balázs Hangya;&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/hangyabalazs/ATS">https://github.com/hangyabalazs/ATS&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Balazs Hangya&lt;/p>
&lt;hr></description></item><item><title>OPETH: Open Source Solution for Real-Time Peri-Event Time Histogram Based on Open Ephys</title><link>https://open-neuroscience.com/post/opeth_open_source_solution_for_real_time_peri_event_time_histogram_based_on_open_ephys/</link><pubDate>Tue, 23 Feb 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/opeth_open_source_solution_for_real_time_peri_event_time_histogram_based_on_open_ephys/</guid><description>&lt;p>Single cell electrophysiology remains one of the most widely used approaches of systems neuroscience. Decisions made by the experimenter during electrophysiology recording largely determine recording quality, duration of the project and value of the collected data. Therefore, online feedback aiding these decisions can lower monetary and time investment, and substantially speed up projects as well as allow novel studies otherwise not possible due to prohibitively low throughput. Real-time feedback is especially important in studies that involve optogenetic cell type identification by enabling a systematic search for neurons of interest. However, such tools are scarce and limited to costly commercial systems with high degree of specialization, which hitherto prevented wide-ranging benefits for the community. To address this, we present an open-source tool that enables online feedback during electrophysiology experiments and provides a Python interface for the widely used Open Ephys open source data acquisition system. Specifically, our software allows flexible online visualization of spike alignment to external events, called the online peri-event time histogram (OPETH). These external events, conveyed by digital logic signals, may indicate photostimulation time stamps for in vivo optogenetic cell type identification or the times of behaviorally relevant events during in vivo behavioral neurophysiology experiments. Therefore, OPETH allows real-time identification of genetically defined neuron types or behaviorally responsive populations. By allowing &amp;ldquo;hunting&amp;rdquo; for neurons of interest, OPETH significantly reduces experiment time and thus increases the efficiency of experiments that combine in vivo electrophysiology with behavior or optogenetic tagging of neurons.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>András Széll; Sergio Martínez-Bellver; Panna Hegedüs; Balázs Hangya&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/hangyabalazs/opeth">https://github.com/hangyabalazs/opeth&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Balazs Hangya&lt;/p>
&lt;hr></description></item><item><title>SignalBuddy</title><link>https://open-neuroscience.com/post/signalbuddy/</link><pubDate>Tue, 23 Feb 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/signalbuddy/</guid><description>&lt;p>SignalBuddy is an easy-to-make, easy-to-use signal generator for scientific applications. Making friends is hard, but making SignalBuddy is easy. All you need is an Arduino Uno! SignalBuddy replaces more complicated and (much) more expensive signal generators in laboratory settings where one millisecond resolution is sufficient.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Richard Warren&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://hackaday.io/project/167649-signalbuddy">https://hackaday.io/project/167649-signalbuddy&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://github.com/richard-warren/SignalBuddy/raw/master/images/SignalBuddy3D.gif">https://github.com/richard-warren/SignalBuddy/raw/master/images/SignalBuddy3D.gif&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Richard Warren&lt;/p>
&lt;hr></description></item><item><title>Open Source Tools for Temporally Controlled Rodent Behavior Suitable for Electrophysiology and Optogenetic Manipulations</title><link>https://open-neuroscience.com/post/open_source_tools_for_temporally_controlled_rodent_behavior_suitable_for_electrophysiology_and_optogenetic_manipulations/</link><pubDate>Fri, 22 Jan 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/open_source_tools_for_temporally_controlled_rodent_behavior_suitable_for_electrophysiology_and_optogenetic_manipulations/</guid><description>&lt;p>Understanding how the brain controls behavior requires observing and manipulating neural activity in awake behaving animals. Neuronal firing is timed at millisecond precision. Therefore, to decipher temporal coding, it is necessary to monitor and control animal behavior at the same level of temporal accuracy. However, it is technically challenging to deliver sensory stimuli and reinforcers as well as to read the behavioral responses they elicit with millisecond precision. Presently available commercial systems often excel in specific aspects of behavior control, but they do not provide a customizable environment allowing flexible experimental design while maintaining high standards for temporal control necessary for interpreting neuronal activity. Moreover, delay measurements of stimulus and reinforcement delivery are largely unavailable. We combined microcontroller-based behavior control with a sound delivery system for playing complex acoustic stimuli, fast solenoid valves for precisely timed reinforcement delivery and a custom-built sound attenuated chamber using high-end industrial insulation materials. Together this setup provides a physical environment to train head-fixed animals, enables calibrated sound stimuli and precisely timed fluid and air puff presentation as reinforcers. We provide latency measurements for stimulus and reinforcement delivery and an algorithm to perform such measurements on other behavior control systems. Combined with electrophysiology and optogenetic manipulations, the millisecond timing accuracy will help interpret temporally precise neural signals and behavioral changes. Additionally, since software and hardware provided here can be readily customized to achieve a large variety of paradigms, these solutions enable an unusually flexible design of rodent behavioral experiments.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Nicola Solari; Katalin Sviatkó; Tamás Laszlovszky; Panna Hegedüs; Balázs Hangya&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/hangyabalazs/Rodent_behavior_setup">https://github.com/hangyabalazs/Rodent_behavior_setup&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Balazs Hangya&lt;/p>
&lt;hr></description></item><item><title>PiVR</title><link>https://open-neuroscience.com/post/pivr/</link><pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/pivr/</guid><description>&lt;p>PiVR is a system that allows experimenters to immerse small animals into virtual realities. The system tracks the position of the animal and presents light stimulation according to predefined rules, thus creating a virtual landscape in which the animal can behave. By using optogenetics, we have used PiVR to present fruit fly larvae with virtual olfactory realities, adult fruit flies with a virtual gustatory reality and zebrafish larvae with a virtual light gradient.&lt;/p>
&lt;p>PiVR operates at high temporal resolution (70Hz) with low latencies (&amp;lt;30 milliseconds) while being affordable (&amp;lt;US$500) and easy to build (&amp;lt;6 hours). Through extensive documentation (&lt;a href="http://www.PiVR.org">www.PiVR.org&lt;/a>), this tool was designed to be accessible to a wide public, from high school students to professional researchers studying systems neuroscience in academia.&lt;/p>
&lt;p>The project is open source (BSD-3) and the documented code written in the freely available programming language Python. We hope that PiVR will be adapted by advanced users for their particular needs, for example to create closed-loop experiments involving other sensory modalities (e.g., sound/vibration) through the use of PWM controllable devices. We envision PiVR to be used as the central module when creating virtual realities for a variety of sensory modalities. This ‘PiVR module’ takes care of detecting the animal and presenting the appropriate PWM signal that is then picked up by the PWM controllable device installed by the user, for example to produce a sound whenever an animal enters a pre-defined region.&lt;/p>
&lt;p>In short, PiVR is a powerful and affordable experimental platform allowing experimenters to create a wide array of virtual reality experiments. Our hope is that PiVR will be adapted by several labs to democratize closed-loop experiments and, by standardizing image quality and the animal detection algorithm, increase reproducibility.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>David Tadres; Matthieu Louis&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="http://www.PiVR.org">http://www.PiVR.org&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=w5tIG6B6FWo">https://www.youtube.com/watch?v=w5tIG6B6FWo&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
David Tadres&lt;/p>
&lt;hr></description></item><item><title>LED Zappelin'</title><link>https://open-neuroscience.com/post/led_zappelin/</link><pubDate>Sun, 05 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/led_zappelin/</guid><description>&lt;p>Two-photon (2P) microscopy is a cornerstone technique in neuroscience research. However, combining 2P imaging with spectrally arbitrary light stimulation can be challenging due to crosstalk between stimulation light and fluorescence detection. To overcome this limitation, we present a simple and low-cost electronic solution based on an ESP32 microcontroller and a TLC5947 LED driver to rapidly time-interleave stimulation and detection epochs during scans. Implemented for less than $100, our design can independently drive up to 24 arbitrary spectrum LEDs to meet user requirements. We demonstrate the utility of our stimulator for colour vision experiments on the in vivo tetrachromatic zebrafish retina and for optogenetic circuit mapping in Drosophila.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Maxime Zimmermann; Andre Maia Chagas; Philipp Bartel; Sinzi Pop, Lucia Pierto Godino; Tom Baden&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/BadenLab/LED-Zappelin">https://github.com/BadenLab/LED-Zappelin&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Maxime Zimmermann&lt;/p>
&lt;hr></description></item><item><title>Addgene's AAV Data Hub</title><link>https://open-neuroscience.com/post/addgenes_aav_data_hub/</link><pubDate>Tue, 23 Jun 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/addgenes_aav_data_hub/</guid><description>&lt;p>AAV are versatile tools used by neuroscientists for expression and manipulation of neurons. Many scientists have benefited from the high-quality, ready-to-use AAV prep service from Addgene, a nonprofit plasmid repository. However, it can be challenging to determine which AAV tool and techniques are best to use for an experiment. Scientists also may have questions about how much virus to inject or which serotype or promoter should be used to target the desired neuron or brain region. To help scientists answer these questions, Addgene launched an open platform called the AAV Data Hub (&lt;a href="https://datahub.addgene.org/aav/">https://datahub.addgene.org/aav/&lt;/a>) which allows researchers to easily share practical experimental details with the scientific community (AAV used, in vivo model used, injection site, injection volumes, etc.). The goal of this platform is to help scientists find the best AAV tool for their experiments by reviewing combined data from a broad range of research labs. The AAV Data Hub launched in late 2019 and over 100 experiments have since been contributed to this project. The dataset includes details and images from experiments conducted in six different species and several different expression sites.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Addgene&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://datahub.addgene.org/aav/">https://datahub.addgene.org/aav/&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=ZPKdr1RdtGI&amp;amp;feature=youtu.be">https://www.youtube.com/watch?v=ZPKdr1RdtGI&amp;amp;feature=youtu.be&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Angela Abitua&lt;/p>
&lt;hr></description></item><item><title>Pulse Pal</title><link>https://open-neuroscience.com/post/pulse-pal/</link><pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/pulse-pal/</guid><description>&lt;blockquote>
&lt;p>Pulse Pal is an open and inexpensive (~$210) alternative to pulse generators used in neurophysiology research, and is most often used to create precisely timed light trains in optogenetics assays. Pulse Pal generates four channels of configurable square pulse trains ranging in voltage from +10 to -10V using a bipolar DAC. Two digital trigger channels can be used to start and stop playback. APIs are available in C++, Python and MATLAB, and the hardware designs and firmware are fully open source.&lt;/p>
&lt;/blockquote>
&lt;p>Be sure to check the &lt;a href="http://journal.frontiersin.org/article/10.3389/fneng.2014.00043/abstract" target="_blank" rel="noopener">paper&lt;/a> about it and their &lt;a href="https://sites.google.com/site/pulsepalwiki/home" target="_blank" rel="noopener">wiki page.&lt;/a>&lt;/p></description></item></channel></rss>