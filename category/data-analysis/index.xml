<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Data Analysis | Open Neuroscience</title><link>https://open-neuroscience.com/category/data-analysis/</link><atom:link href="https://open-neuroscience.com/category/data-analysis/index.xml" rel="self" type="application/rss+xml"/><description>Data Analysis</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>CC BY SA 4.0</copyright><lastBuildDate>Sun, 28 Mar 2021 00:00:00 +0000</lastBuildDate><image><url>https://open-neuroscience.com/media/openneuroscience_logo_dark.svg</url><title>Data Analysis</title><link>https://open-neuroscience.com/category/data-analysis/</link></image><item><title>CellExplorer - Framework for analyzing single cells</title><link>https://open-neuroscience.com/post/cellexplorer_framework_for_analyzing_single_cells/</link><pubDate>Sun, 28 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/cellexplorer_framework_for_analyzing_single_cells/</guid><description>&lt;p>The large diversity of cell-types of the brain, provides the means by which circuits perform complex operations. Understanding such diversity is one of the key challenges of modern neuroscience. Neurons have many unique electrophysiological and behavioral features from which parallel cell-type classification can be inferred.&lt;/p>
&lt;p>To address this, we built CellExplorer, a framework for analyzing and characterizing single cells recorded using extracellular electrodes. It can be separated into three components: a standardized yet flexible data structure, a single yet extensive processing module, and a powerful graphical interface. Through the processing module, a high dimensional representation is built from electrophysiological and functional features including the spike waveform, spiking statistics, monosynaptic connections, and behavioral spiking dynamics. The user-friendly interactive graphical interface allows for classification and exploration of those features, through a rich set of built-in plots, interaction modes, cell grouping, and filters. Powerful figures can be created for publications. Opto-tagged cells and public access to reference data have been incorporated to help you characterize your data better. The framework is built entirely in MATLAB making it fast and intuitive to implement and incorporate CellExplorer into your pipelines and analysis scripts. You can expand it with your metrics, plots, and opto-tagged data.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Peter C. Petersen, Joshua H. Siegle, Nicholas A. Steinmetz, Sara Mahallati, György Buzsáki&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://cellexplorer.org/">https://cellexplorer.org/&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=GR1glNhcGIY">https://www.youtube.com/watch?v=GR1glNhcGIY&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Peter C. Petersen&lt;/p>
&lt;hr></description></item><item><title>BrainGlobe</title><link>https://open-neuroscience.com/post/brainglobe/</link><pubDate>Thu, 25 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/brainglobe/</guid><description>&lt;p>BrainGlobe is a suite of Python-based computational neuroanatomy software tools. We provide software packages for the analysis and visualisation of neuroanatomical data, particularly from whole-brain microscopy. In addition, we provide tools for working with brain atlases, to simplify development of new tools and aid collaboration and cooperation by adopting common standards.&lt;/p>
&lt;p>Our tools include:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://open-neuroscience.com/post/brainglobe_atlas_api/" target="_blank" rel="noopener">BrainGlobe Atlas API&lt;/a> - A lightweight python module to interact with atlases for systems neuroscience&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://open-neuroscience.com/post/cellfinder/" target="_blank" rel="noopener">Cellfinder&lt;/a> - Automated 3D cell detection and registration of whole-brain microscopy images&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://open-neuroscience.com/post/brainrender/" target="_blank" rel="noopener">Brainrender&lt;/a> - A Python based software for visualization of neuroanatomical and morphological data.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Project Author(s): Adam Tyson; Federico Claudi; Luigi Petrucco&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/brainglobe">https://github.com/brainglobe&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Adam Tyson&lt;/p>
&lt;hr></description></item><item><title>Kilosort</title><link>https://open-neuroscience.com/post/kilosort/</link><pubDate>Sun, 21 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/kilosort/</guid><description>&lt;p>Kilosort is a software package for identifying neurons and their spikes in extracellular electrophysiology, a process known as &amp;ldquo;spike sorting&amp;rdquo;. Kilosort has been primarily developed and tested on the Neuropixels 1.0 probes, but is now used on data acquired with a wide variety of ephys methods, from Utah arrays to tetrodes and to the latest Neuropixels 2.0 probes.&lt;/p>
&lt;p>Spike sorting consists of several largely-independent steps: data preprocessing, drift correction, spike clustering, template matching, and results postprocessing. There have been several versions of Kilosort, improving on various aspects of these steps, and we are currently on version v3. In many cases, and especially for Neuropixels probes, the automated output of Kilosort3 requires minimal manual curation. The main change from v2.5 is a completely new and much more sophisticated clustering algorithm. To learn about Kilosort2.5, the primary reference is the Neuropixels 2.0 paper. Kilosort2.5 improves on Kilosort2 primarily in the type of drift correction we use. Where Kilosort2 modified templates as a function of time/drift (a drift tracking approach), Kilosort2.5 corrects the raw data directly via a sub-pixel registration process (a drift correction approach).&lt;/p>
&lt;p>All versions of Kilosort so far have been developed and released in Matlab. However, we are advancing towards Python releases of the codebase, including of older version like Kilosort 2 and 2.5 (available here: &lt;a href="https://github.com/MouseLand/pykilosort/tree/master/pykilosort)">https://github.com/MouseLand/pykilosort/tree/master/pykilosort)&lt;/a>.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Marius Pachitariu&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/MouseLand/Kilosort">https://github.com/MouseLand/Kilosort&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://youtu.be/clq50N7V_wA">https://youtu.be/clq50N7V_wA&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Marius Pachitariu&lt;/p>
&lt;hr></description></item><item><title>Suite2P</title><link>https://open-neuroscience.com/post/suite2p/</link><pubDate>Tue, 16 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/suite2p/</guid><description>&lt;p>Suite2P is a very modular imaging processing pipeline written in Python which allows you to perform registration of raw data movies, automatic cell detection, extraction of calcium traces and infers spike times. It is a very fast and accurate tool and can work on standard workstations. It also includes a visualization graphical user interface (GUI) that facilitates analysis and manual curation of the cell detection algorithm.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Carsen Stringer and Marius Pachitariu&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://mouseland.github.io/suite2p/_build/html/index.html">https://mouseland.github.io/suite2p/_build/html/index.html&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Miguel Fernandes&lt;/p>
&lt;hr></description></item><item><title>BigPint Bioconductor package that makes BIG (RNA seq) data pint sized</title><link>https://open-neuroscience.com/post/bigpint_bioconductor_package_that_makes_big_rna_seq_data_pint_sized/</link><pubDate>Sat, 13 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/bigpint_bioconductor_package_that_makes_big_rna_seq_data_pint_sized/</guid><description>&lt;p>The BigPint package can help examine any large multivariate dataset. However, we note that the example datasets and example code in this package consider RNA-sequencing datasets. If you are using this software for RNA-sequencing data, then it can help you confirm that the variability between your treatment groups is larger than that between your replicates and determine how various normalization techniques in popular RNA-sequencing analysis packages (such as edgeR, DESeq2, and limma) affect your dataset. Moreover, you can easily superimpose lists of differentially expressed genes (DEGs) onto your dataset to check that they show the expected patterns (large variability between treatment groups and small variability between replicates).&lt;/p>
&lt;ol>
&lt;li>
&lt;p>BigPint software website: &lt;a href="https://lindsayrutter.github.io/bigPint/">https://lindsayrutter.github.io/bigPint/&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Article explaining the BigPint methodology: &lt;a href="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-2968-1">https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-2968-1&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Research article showcasing the BigPint software:
&lt;a href="https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-019-5767-1">https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-019-5767-1&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Article explaining the BigPint software: &lt;a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007912">https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007912&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Lindsay Rutter; Dianne Cook&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/lindsayrutter/bigPint">https://github.com/lindsayrutter/bigPint&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Anonymous&lt;/p>
&lt;hr></description></item><item><title>Collaborative Informatics and Neuroimaging Suite Toolkit for Anonymous Computation (COINSTAC)</title><link>https://open-neuroscience.com/post/collaborative_informatics_and_neuroimaging_suite_toolkit_for_anonymous_computation_coinstac_/</link><pubDate>Sat, 13 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/collaborative_informatics_and_neuroimaging_suite_toolkit_for_anonymous_computation_coinstac_/</guid><description>&lt;p>COINSTAC provides a platform to analyze data stored locally across multiple organizations without the need for pooling the data at any point during the analysis. It is intended to be an ultimate one-stop shop by which researchers can build any statistical or machine learning model collaboratively in a decentralized fashion. This framework implements a message passing infrastructure that will allow large scale analysis of decentralized data with results on par with those that would have been obtained if the data were in one place.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Sergey M. Plis; Anand D. Sarwate; Dylan Wood; Christopher Dieringer; Drew Landis; Cory Reed; Sandeep R. Panta; Jessica A. Turner; Jody M. Shoemaker; Kim W. Carter; Paul Thompson; Kent Hutchison; Vince D. Calhoun&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/trendscenter/coinstac">https://github.com/trendscenter/coinstac&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Kelly Rootes-Murdy&lt;/p>
&lt;hr></description></item><item><title>EASI-FISH</title><link>https://open-neuroscience.com/post/easi_fish/</link><pubDate>Sat, 13 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/easi_fish/</guid><description>&lt;p>This workflow is used to analyze large-scale, multi-round, high-resolution image data acquired using EASI-FISH (Expansion-Assisted Iterative Fluorescence In Situ Hybridization). It takes advantage of the n5 filesystem to allow for rapid and parallel data reading and writing. It performs automated image stitching, distributed and highly accurate multi-round image registration, 3D cell segmentation, and distributed spot detection. We also envision this workflow being adapted for analysis of other image-based spatial transcriptomic data.&lt;/p>
&lt;p>Check the pre-print here &lt;a href="https://www.biorxiv.org/content/10.1101/2021.03.08.434304v1">https://www.biorxiv.org/content/10.1101/2021.03.08.434304v1&lt;/a>&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Yuhan Wang; Konrad Rokicki; Avatar Cristian Goina&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/multiFISH/EASI-FISH">https://github.com/multiFISH/EASI-FISH&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matias Andina&lt;/p>
&lt;hr></description></item><item><title>MNE-Python</title><link>https://open-neuroscience.com/post/mne_python/</link><pubDate>Sat, 13 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/mne_python/</guid><description>&lt;p>MNE-Python is an open-source Python module for neuroscience data analysis. It implements many neuroscience-specific algorithms and statistical tools; has rich visualization capabilities that are both interactive and fully scriptable (for reproducibility); and integrates easily with general-purpose libraries like SciPy, Scikit-learn, and TensorFlow.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Eric Larson; Alexandre Gramfort; Denis A. Engemann; Jaakko Leppakangas; Christian Brodbeck; Mainak Jas; Teon Brooks; Jona Sassenhagen; Martin Luessi; Jean-Remi King; Roman Goj; Daniel McCloy; Mark Wronkiewicz; Marijn van Vliet; Guillaume Favelier; Clemens Brunner; Chris Holdgraf; Joan Massich; Yousra Bekhti; Richard Höchenberger; Alan Leggitt; Andrew Dykstra; Romain Trachel; Lorenzo De Santis; Asish Panda; Stefan Appelhoff; Mikołaj Magnuski; Martin Billinger; Britta Westner; Dan G. Wakeman; Daniel Strohmeier; Robert Luke; Hari Bharadwaj; Tal Linzen; Alexandre Barachant; Emily Ruzich; Christopher J. Bailey; Clément Moutard; Luke Bloy; Fede Raimondo; Jussi Nurminen; Jair Montoya; Marmaduke Woodman; Ingoo Lee; Nick Foti; Cathy Nangini; José C. García Alanis; Roan LaPlante; Ross Maddox; Christoph Dinh; Olaf Hauk; Adam Li; Guillaume Dumas; Paul Pasler; Stefan Repplinger; Thomas Hartmann; Alexander Rudiuk; Brad Buran; Mathurin Massias; Matti Hämäläinen; Praveen Sripad; Christopher Mullins; Félix Raimundo; Phillip Alday; Simon Kornblith; Yaroslav Halchenko; Yu-Han Luo; Johannes Kasper; Keith Doelling; Mads Jensen; Tanay Gahlot; Adonay Nunes; Dirk Gütlin; kjs; Alejandro Weinstein; Camilo Lamus; Cristóbal Moënne-Loccoz; Natalie Klein; Alex Rockhill; Antti Rantala; Burkhard Maess; Erkka Heinila; Henrich Kolkhorst; Jeff Hanna; Jon Houck; Saket Choudhary; Christian O&amp;rsquo;Reilly; Fu-Te Wong; Hubert Banville; Ivana Kojcic; Jesper Duemose Nielsen; Kaisu Lankinen; Kambiz Tabavi; Kostiantyn Maksymenko; Louis Thibault; Nathalie Gayraud; Nick Ward; Antoine Gauthier; Basile Pinsard; Emily Stephen; Erik Hornberger; Evgenii Kalenkovich; Fahimeh Mamashli; Hafeza Anevar; Johann Benerradi; Larry Eisenman; Lorenz Esch; Nicolas Barascud; Nicolas Legrand; Samuel Deslauriers-Gauthier; Simon Kern; Victor Férat; Alexander Kovrig; Annalisa Pascarella; Dominik Krzemiński; Ezequiel Mikulan; Jean-Baptiste Schiratti; Jen Evans; Kyle Mathewson; Laura Gwilliams; Lenny Varghese; Lx37; Martin Schulz; Matt Boggess; Mohamed Sherif; Nataliia Kozhemiako; Niklas Wilming; Oleh Kozynets; Pierre Ablin; Quentin Bertrand; Rodrigo Hübner; Sara Sommariva; Sheraz Khan; Sophie Herbst; Thomas Jochmann; Tod Flak; Tom Dupré la Tour; Tristan Stenner; akshay0724; sviter; Abram Hindle; Achilleas Koutsou; Aniket Pradhan; Anne-Sophie Dubarry; Anton Nikolas Waniek; Ariel Rokem; Austin Hurst; Bruno Nicenboim; Carlos de la Torre; Christian Clauss; Chun-Hui Li; Claire Braboszcz; David Haslacher; David Sabbagh; Demetres Kostas; Desislava Petkova; Dmitrii Altukhov; Dominik Welke; Eberhard Eich; Eduard Ort; Elizabeth DuPre; Ellen Lau; Emanuele Olivetti; Evan Hathaway; Geoff Brookshire; Hermann Sonntag; Hongjiang Ye; Jakub Kaczmarzyk; Jasper J.F. van den Bosch; Jeff Stout; Jeroen Van Der Donckt; Johan van der Meer; Johannes Niediek; Joshua J Bear; Juergen Dammers; Katarina Slama; Katrin Leinweber; Laetitia Grabot; Lau Møller Andersen; Leonardo S. Barbosa; Liberty Hamilton; Lorenzo Alfine; Lukáš Hejtmánek; Manfred Kitzbichler; Manoj Kumar; Manu Sutela; Marcin Koculak; Marian Dovgialo; Martin van Harmelen; MartinBaBer; Matt Tucker; Matteo Visconti di Oleggio Castello; Michael Krause; Milan Rybář; Mohammad Daneshzand; Nicole Proulx; Nikolas Chalas; Padma Sundaram; Paul Roujansky; Pedro Silva; Peter J. Molfese; Quanliang Li; Rahul Nadkarni; Ramiro Gatti; Ramonapariciog Apariciogarcia; Richard Koehler; Robert Oostenveld; Robert Seymour; Robin Tibor Schirrmeister; Sagun Pai; Sam Perry; Sebastian Major; Sebastián Castaño; Sergey Antopolskiy; Simeon Wong; Simon-Shlomo Poil; Sourav Singh; Stanislas Chambon; Steve Matindi; Steven Bethard; Steven Bierer; Steven M. Gutstein; Svea Marie Meyer; Theodore Papadopoulo; Thomas Donoghue; Thomas Radman; Tommy Clausner; ZHANG Zhi; apadee; buildqa; chapochn; enricovara; mshader&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://mne.tools">https://mne.tools&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Daniel McCloy&lt;/p>
&lt;hr></description></item><item><title>napari</title><link>https://open-neuroscience.com/post/napari/</link><pubDate>Sat, 13 Mar 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/napari/</guid><description>&lt;p>Napari is a fast, interactive, multi-dimensional image viewer for Python. It’s designed for browsing, annotating, and analyzing large multi-dimensional images. It’s built on top of Qt (for the GUI), vispy (for performant GPU-based rendering), and the scientific Python stack (numpy, scipy).&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>napari team&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://napari.org/">https://napari.org/&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Miguel Fernandes&lt;/p>
&lt;hr></description></item><item><title>CLIJ2</title><link>https://open-neuroscience.com/post/clij2/</link><pubDate>Tue, 23 Feb 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/clij2/</guid><description>&lt;p>CLIJ2 is a GPU-accelerated image processing library for ImageJ/Fiji, Icy, Matlab and Java based on OpenCL. It comes with hundreds of operations for filtering, binarizing, labeling, measuring in images, projections, transformations and mathematical operations for images.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Robert Haase&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://clij.github.io/">https://clij.github.io/&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Miguel Fernandes&lt;/p>
&lt;hr></description></item><item><title>morphologica</title><link>https://open-neuroscience.com/post/morphologica/</link><pubDate>Tue, 23 Feb 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/morphologica/</guid><description>&lt;p>morphologica is a header-only C++ library which provides simulation support facilities for simulations of dynamical systems.&lt;/p>
&lt;p>It helps with:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Configuration: morphologica allows you to easily set up a simulation parameter configuration system, using the JSON reading and writing abilities of morph::Config.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Saving data from your simulation. morphologica provides a set of easy-to-use convenience wrappers (morph::HdfData) around the HDF5 C API. By saving data in a standard format, it is easy to access simulation data in python, MATLAB or Octave for analysis and graphing.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Visualizing your model while it runs. A modern OpenGL visualization scheme called morph::Visual provides the ability to visualise hex &amp;amp; Cartesian grids, surfaces, scatter plots and quiver plots with minimal processing overhead.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Seb James; Stuart Wilson&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/ABRG-Models/morphologica">https://github.com/ABRG-Models/morphologica&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://youtu.be/UejRHe-6LoM">https://youtu.be/UejRHe-6LoM&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Seb James&lt;/p>
&lt;hr></description></item><item><title>NetPyNE</title><link>https://open-neuroscience.com/post/netpyne/</link><pubDate>Fri, 22 Jan 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/netpyne/</guid><description>&lt;p>NetPyNE (Networks using Python and NEURON) is a Python package to facilitate the development, simulation, parallelization, analysis, and optimization of biophysical neuronal networks using the NEURON simulator.&lt;/p>
&lt;p>For more details, installation instructions, documentation, tutorials, forums, videos and more, please visit: &lt;a href="http://www.netpyne.org">www.netpyne.org&lt;/a>&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Salvador Dura-Bernal&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/Neurosim-lab/netpyne">https://github.com/Neurosim-lab/netpyne&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://youtu.be/TV7ipuDHBd0">https://youtu.be/TV7ipuDHBd0&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Joe Graham&lt;/p>
&lt;hr></description></item><item><title>Greedy</title><link>https://open-neuroscience.com/post/greedy/</link><pubDate>Wed, 13 Jan 2021 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/greedy/</guid><description>&lt;p>Greedy is a tool for fast medical image registration. It is a fast CPU-based deformable image registration tool that can be used in applications where many images have to be registered in parallel. It can perform affine and rigid image (2D and 3D data) registration. You can also supply masks for restricting registration to specific regions.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Paul Yushkevich&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://greedy.readthedocs.io/en/latest/index.html">https://greedy.readthedocs.io/en/latest/index.html&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Miguel Fernandes&lt;/p>
&lt;hr></description></item><item><title>ERPLAB</title><link>https://open-neuroscience.com/post/erplab/</link><pubDate>Thu, 19 Nov 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/erplab/</guid><description>&lt;p>ERPLAB Toolbox is a free, open-source Matlab package for analyzing ERP data. It is tightly integrated with EEGLAB Toolbox, extending EEGLAB’s capabilities to provide robust, industrial-strength tools for ERP processing, visualization, and analysis. A graphical user interface makes it easy for beginners to learn, and Matlab scripting provides enormous power for intermediate and advanced users.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Andrew X Stewart; Steve Luck&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/lucklab/erplab">https://github.com/lucklab/erplab&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Andrew X Stewart&lt;/p>
&lt;hr></description></item><item><title>TGAC Browser</title><link>https://open-neuroscience.com/post/tgac_browser/</link><pubDate>Thu, 19 Nov 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/tgac_browser/</guid><description>&lt;p>The TGAC Browser is a Genomic Browser with novel rendering and annotation capabilities designed to overcome some shortcomings in available approaches. It was developed to visualize genome annotations from Ensembl Database Schema.&lt;/p>
&lt;p>The TGAC Browser provides the following features:&lt;/p>
&lt;p>Responsiveness: Client-side rendering and caching, based on JSON fragments generated by server logic, helps decrease the server load and improves user experience.&lt;/p>
&lt;p>User-friendly browser Interaction: Live data searching, track modification, and drag and drop selection; actions that are seamlessly powered by modern web browsers.&lt;/p>
&lt;p>Analysis Integration: The ability to carry out heavyweight analysis tasks, using tools such as BLAST, via a dedicated extensible daemon service.&lt;/p>
&lt;p>User Annotation: Users can edit annotations which can be persisted on the server, reloaded, and shared at a later date.&lt;/p>
&lt;p>Off-the-shelf Installation: The only prerequisites are a web application container, such as Jetty or Tomcat, and a standard Ensembl database to host sequence features.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Anil S. Thanki; Xingdong Bian; Robert P. Davey&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="http://browser.tgac.ac.uk/">http://browser.tgac.ac.uk/&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Miguel Fernandes&lt;/p>
&lt;hr></description></item><item><title>BrainSMASH</title><link>https://open-neuroscience.com/post/brainsmash/</link><pubDate>Tue, 06 Oct 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/brainsmash/</guid><description>&lt;p>BrainSMASH is a Python-based framework for quantifying the significance of a brain map’s spatial topography in studies of large-scale brain organization. BrainSMASH was designed to generate synthetic brain maps with spatial autocorrelation (SA) matched to the SA of a target brain map.&lt;/p>
&lt;p>SA is a prominent and ubiquitous property of brain maps that violates the assumptions of independence/exchangeability that underlie many conventional statistical tests. Controlling for this property is therefore necessary to disambiguate meaningful topographic relationships from chance associations. To this end, BrainSMASH instantiates a generative null model for simulating surrogate brain maps, constrained by empirical data, that preserve the SA of cortical (surface-based), subcortical (volumetric), parcellated, and dense brain maps.&lt;/p>
&lt;p>BrainSMASH requires only two inputs: a brain map of interest, and a matrix of pairwise distances between elements of the brain map. How these inputs are derived is left to user discretion, though additional support has been provided for investigators working with HCP-compliant neuroimaging files. Specifically, BrainSMASH includes routines to generate two-dimensional Euclidean and geodesic distance matrices from surface geometry (GIFTI) files, and subcortical Euclidean distance matrices from CIFTI-format neuroimaging files.&lt;/p>
&lt;p>Detailed documentation for BrainSMASH can be found at &lt;a href="https://brainsmash.readthedocs.io/">https://brainsmash.readthedocs.io/&lt;/a>.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Joshua B. Burt; Markus Helmer; Maxwell Shinn; Alan Anticevic; John D. Murray&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/murraylab/brainsmash">https://github.com/murraylab/brainsmash&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Joshua B. Burt&lt;/p>
&lt;hr></description></item><item><title>FastTrack</title><link>https://open-neuroscience.com/post/fasttrack/</link><pubDate>Wed, 30 Sep 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/fasttrack/</guid><description>&lt;p>FastTrack is an open-source cross-platform tracking software. Easy to install and easy to use, it can track a large variety of systems from active particles to animals, with a known or unknown number of objects. It can process movies from any quality on low-end to high-end computers.&lt;/p>
&lt;p>Two main features are implemented in the software:
- A fast and automatic tracking algorithm that can detect and track objects, conserving the objects' identities across the video recording.
- A manual tool to review the tracking where errors can be corrected rapidly and easily to achieve 100% accuracy with a minimum of efforts.&lt;/p>
&lt;p>FastTrack do not require coding abilities to be used. A developer documentation is available for users who want to embed FastTrack tracking algorithm directly inside their projects.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Benjamin Gallois&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/FastTrackOrg/FastTrack">https://github.com/FastTrackOrg/FastTrack&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="http://www.fasttrack.sh/UserManual/docs/assets/example_vid.webm">http://www.fasttrack.sh/UserManual/docs/assets/example_vid.webm&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Benjamin Gallois&lt;/p>
&lt;hr></description></item><item><title>INCF Portal</title><link>https://open-neuroscience.com/post/incf_portal/</link><pubDate>Wed, 30 Sep 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/incf_portal/</guid><description>&lt;p>The INCF portal is the guide to the INCF activities and its community resources.&lt;/p>
&lt;p>INCF advances data reuse and reproducibility in brain research by coordinating the development of Open, FAIR, and Citable tools and resources for neuroscience&lt;/p>
&lt;p>The global INCF network&lt;/p>
&lt;ul>
&lt;li>supports the development of neuroinformatics as a discipline&lt;/li>
&lt;li>moves neuroscience towards FORCE (a FAIR, Open, Research-object based, Citable Ecosystem)&lt;/li>
&lt;li>provides coordination of global neuroscience infrastructure through the development and endorsement of consensus-based standards and best practices&lt;/li>
&lt;li>trains scientists, administrators, and students in using neuroinformatics tools and methods&lt;/li>
&lt;/ul>
&lt;p>INCF provides materials, expertise, training, and standards and best practices for: 1) scientists seeking to improve their science through neuroinformatics, 2) infrastructure providers so they can do their jobs better and participate in the global network, and 3) those seeking to add their tools, services, and expertise to the INCF network.&lt;/p>
&lt;p>INCF currently has Governing and Associate Nodes spanning 4 continents, with an extended network comprising organizations, individual researchers, industry, and publishers. The INCF Secretariat supports the Nodes with outreach, project management, and administration of community-driven projects.&lt;/p>
&lt;p>You can join the INCF network: &lt;a href="https://www.incf.org/join-incf">https://www.incf.org/join-incf&lt;/a>&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>INCF team&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://www.incf.org">https://www.incf.org&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=teFJ_W6nVUY">https://www.youtube.com/watch?v=teFJ_W6nVUY&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Malin Sandström&lt;/p>
&lt;hr></description></item><item><title>Open Source Brain</title><link>https://open-neuroscience.com/post/open_source_brain/</link><pubDate>Tue, 29 Sep 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/open_source_brain/</guid><description>&lt;p>Open Source Brain, a platform for sharing, viewing, analyzing, and simulating standardized models from different brain regions and species.&lt;/p>
&lt;p>Model structure and parameters can be automatically visualized and their dynamical properties explored through browser-based simulations.&lt;/p>
&lt;p>Infrastructure and tools for collaborative interaction, development, and testing are also provided.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Open Source Brain contributors&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="http://www.opensourcebrain.org/">http://www.opensourcebrain.org/&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Ankur Sinha (@ OSB/University College London)&lt;/p>
&lt;hr></description></item><item><title>MorphoPy: A python package for feature extraction of neural morphologies</title><link>https://open-neuroscience.com/post/morphopy_a_python_package_for_feature_extraction_of_neural_morphologies/</link><pubDate>Mon, 28 Sep 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/morphopy_a_python_package_for_feature_extraction_of_neural_morphologies/</guid><description>&lt;p>MorphoPy is an open software package written in Python3 that allows for visualization and processing of morphological reconstructions of neural data. It has been created to facilitate the translation from morphology graphs into descriptive features like density maps, morphometric statistics, and persistence diagrams for down-stream exploration and statistical analysis.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Sophie Laturnus; Adam von Daranyi; Ziwei Huang; Philipp Berens&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/berenslab/MorphoPy">https://github.com/berenslab/MorphoPy&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Sophie Laturnus&lt;/p>
&lt;hr></description></item><item><title>OpenCitations</title><link>https://open-neuroscience.com/post/opencitations/</link><pubDate>Sun, 27 Sep 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/opencitations/</guid><description>&lt;p>OpenCitations is an independent infrastructure organization for open scholarship dedicated to the publication of open bibliographic and citation data by the use of Semantic Web (Linked Data) technologies. It is also engaged in advocacy for open citations, particularly by organizing the Workshops for Open Citations and Scholarly Metadata, and in its role as a key founding member of the Initiative for Open Citations (I4OC) and the Initiative for Open Abstracts (I4OA).&lt;/p>
&lt;p>OpenCitations espouses fully the founding principles of Open Science. It complies with the FAIR data principles by Force11 that data should be findable, accessible, interoperable and re-usable, and it complies with the recommendations of I4OC that citation data in particular should be structured, separable, and open. On the latter topic, OpenCitations has recently published a formal definition of an Open Citation, and has launched a system for globally unique and persistent identifiers (PIDs) for bibliographic citations – Open Citation Identifiers (OCIs).&lt;/p>
&lt;p>The following publication is the canonical publication describing OpenCitations itself, as an infrastructure organization for open scholarship. The article describes OpenCitations and its datasets, tools, services and activities.&lt;/p>
&lt;p>Silvio Peroni, David Shotton (2020). OpenCitations, an infrastructure organization for open scholarship. Quantitative Science Studies, 1(1): 428-444. &lt;a href="https://doi.org/10.1162/qss_a_00023">https://doi.org/10.1162/qss_a_00023&lt;/a>&lt;/p>
&lt;p>The OpenCitations Data Model (OCDM) is the metadata model used for the data stored in all the OpenCitations' datasets, described in&lt;/p>
&lt;p>Marilena Daquino, Silvio Peroni, David Shotton (2020). The OpenCitations Data Model. Figshare. &lt;a href="https://doi.org/10.6084/m9.figshare.3443876">https://doi.org/10.6084/m9.figshare.3443876&lt;/a>.&lt;/p>
&lt;p>The largest dataset created by OpenCitations is COCI, the OpenCitations Index of Crossref open DOI-to-DOI citations, an RDF dataset containing details of all the citations that are specified by the open references to DOI-identified works present in Crossref. COCI does not index Crossref references that are not open, nor Crossref open references to entities that lack DOIs. The citations available in COCI are treated as first-class data entities, with accompanying properties including the citations timespan, modelled according to the OpenCitations Data Model.&lt;/p>
&lt;p>Currently, COCI contains information concerning 733,367,140 citations, and 59,455,917 bibliographic resources. COCI was most recently updated on 6 September 2020.&lt;/p>
&lt;p>For an in-depth description of COCI, see:&lt;/p>
&lt;p>Ivan Heibi, Silvio Peroni, David Shotton (2019). Software review: COCI, the OpenCitations Index of Crossref open DOI-to-DOI citations. Scientometrics, 121 (2): 1213-1228. &lt;a href="https://doi.org/10.1007/s11192-019-03217-6">https://doi.org/10.1007/s11192-019-03217-6&lt;/a>&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>David Shotton; Silvio Peroni&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="http://opencitations.net/">http://opencitations.net/&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=-bCPS2iIdCc&amp;amp;feature=youtu.be">https://www.youtube.com/watch?v=-bCPS2iIdCc&amp;amp;feature=youtu.be&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
David Shotton&lt;/p>
&lt;hr></description></item><item><title>DIPY</title><link>https://open-neuroscience.com/post/dipy/</link><pubDate>Sat, 26 Sep 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/dipy/</guid><description>&lt;p>DIPY is the paragon 3D/4D+ imaging library in Python. Contains generic methods for spatial normalization, signal processing, machine learning, statistical analysis and visualization of medical images. Additionally, it contains specialized methods for computational anatomy including diffusion, perfusion and structural imaging.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>&lt;a href="https://github.com/dipy/dipy/graphs/contributors">https://github.com/dipy/dipy/graphs/contributors&lt;/a>&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://dipy.org">https://dipy.org&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/channel/UCHnEuCRDGFOR5cfEo0nD3pw">https://www.youtube.com/channel/UCHnEuCRDGFOR5cfEo0nD3pw&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
anonymous&lt;/p>
&lt;hr></description></item><item><title>BrainGlobe atlas API</title><link>https://open-neuroscience.com/post/brainglobe_atlas_api/</link><pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/brainglobe_atlas_api/</guid><description>&lt;p>Many excellent brain atlases exist for different species. Some of them have an API (application programming interface) to allow users to interact with the data programmatically (e.g. the excellent Allen Mouse Brain Atlas), but many do not, and there is no consistent way to process data from multiple sources.&lt;/p>
&lt;p>The brainglobe atlas API (BG-AtlasAPI) deals with this problem by providing a common interface for programmers to download and process atlas data from multiple sources.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Adam Tyson; Federico Claudi; Luigi Petrucco&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/brainglobe/bg-atlasapi">https://github.com/brainglobe/bg-atlasapi&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Adam Tyson&lt;/p>
&lt;hr></description></item><item><title>brainrender</title><link>https://open-neuroscience.com/post/brainrender/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/brainrender/</guid><description>&lt;p>brainrender is a python package for the visualization of three dimensional neuro-anatomical data. It can be used to render data from publicly available data set (e.g. Allen Brain atlas) as well as user generated experimental data. The goal of brainrender is to facilitate the exploration and dissemination of neuro-anatomical data by providing a user-friendly platform to create high-quality 3D renderings.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Federico Claudi&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/brainglobe/brainrender">https://github.com/brainglobe/brainrender&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Federico Claudi&lt;/p>
&lt;hr></description></item><item><title>JASP</title><link>https://open-neuroscience.com/post/jasp/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/jasp/</guid><description>&lt;p>JASP is a cross-platform statistical software program with a state-of-the-art graphical user interface. The JASP interface allows you to conduct statistical analyses in seconds, and without having to learn programming or risking a programming mistake. JASP is open-source and free of charge, and we provide it as a service to the community. JASP is statistically inclusive as it offers both frequentist and Bayesian analysis methods.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>The JASP Team&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://jasp-stats.org/">https://jasp-stats.org/&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=HxqB7CUA-XI">https://www.youtube.com/watch?v=HxqB7CUA-XI&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
EJ Wagenmakers&lt;/p>
&lt;hr></description></item><item><title>Neuroimaging Informatics Tools and Resources Collaboratory (NITRC)</title><link>https://open-neuroscience.com/post/neuroimaging_informatics_tools_and_resources_collaboratory_nitrc/</link><pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/neuroimaging_informatics_tools_and_resources_collaboratory_nitrc/</guid><description>&lt;p>NeuroImaging Tools &amp;amp; Resources Collaboratory is an award-winning free web-based resource that offers comprehensive information on an ever expanding scope of neuroinformatics software and data. Since debuting in 2007, NITRC has helped the neuroscience community make further discoveries using software and data produced from research that used to end up lost or disregarded.&lt;/p>
&lt;p>NITRC also provides free access to data and enables pay-per-use cloud-based access to unlimited computing power, enabling worldwide scientific collaboration with minimal startup and cost. NITRC’s scientific focus includes: MR, PET/SPECT, CT, EEG/MEG, optical imaging, clinical neuroimaging, computational neuroscience, and imaging genomics software tools, data, and computational resources.&lt;/p>
&lt;p>With NITRC and its components—the Resources Registry (NITRC-R), Image Repository (NITRC-IR), and Computational Environment (NITRC-CE)—a researcher can obtain pilot or proof-of-concept data to validate a hypothesis for just a few dollars.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>NITRC Development Team&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="http://www.nitrc.org">http://www.nitrc.org&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
David Kennedy&lt;/p>
&lt;hr></description></item><item><title>ReproNim: A Center for Reproducible Neuroimaging Computation</title><link>https://open-neuroscience.com/post/repronim_a_center_for_reproducible_neuroimaging_computation/</link><pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/repronim_a_center_for_reproducible_neuroimaging_computation/</guid><description>&lt;p>ReproNim&amp;rsquo;s goal is to improve the reproducibility of neuroimaging science and extend the value of our national investment in neuroimaging research, while making the process easier and more efficient for investigators.&lt;/p>
&lt;p>ReproNim delivers a reproducible analysis framework comprised of components that include: 1) data and software discovery; 2) implementation of standardized description of data, results and workflows; 3) development of execution options that facilitates operation in all computational environments; 4)
provision of training and education to the community.&lt;/p>
&lt;p>All components of the framework are intended to foster continued use and development of the reproducible and generalizable framework in neuroimaging research.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>The ReproNim Development Team&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/ReproNim">https://github.com/ReproNim&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
David Kennedy&lt;/p>
&lt;hr></description></item><item><title>Simple Behavioral Analysis (SimBA)</title><link>https://open-neuroscience.com/post/simple_behavioral_analysis_simba/</link><pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/simple_behavioral_analysis_simba/</guid><description>&lt;p>Several excellent computational frameworks exist that enable high-throughput and consistent tracking of freely moving unmarked animals. SimBA introduce and distribute a plug-and play pipeline that enables users to use these pose-estimation approaches in combination with behavioral annotation for the generation of supervised machine-learning behavioral predictive classifiers.&lt;/p>
&lt;p>SimBA was developed for the analysis of complex social behaviors, but includes the flexibility for users to generate predictive classifiers across other behavioral modalities with minimal effort and no specialized computational background.&lt;/p>
&lt;p>SimBA has a variety of extended functions for large scale batch video pre-processing, generating descriptive statistics from movement features, and interactive modules for user-defined regions of interest and visualizing classification probabilities and movement patterns.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Simon Nilsson: Jia Jie Chhong; Sophia Hwang; Nastacia Goodwin; Sam A Golden&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/sgoldenlab/simba">https://github.com/sgoldenlab/simba&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=Frq6mMcaHBc&amp;amp;list=PLi5Vwf0hhy1R6NDQJ3U28MOUJPfl2YWYl&amp;amp;index=2&amp;amp;t=0s">https://www.youtube.com/watch?v=Frq6mMcaHBc&amp;amp;list=PLi5Vwf0hhy1R6NDQJ3U28MOUJPfl2YWYl&amp;amp;index=2&amp;amp;t=0s&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Simon Nilsson&lt;/p>
&lt;hr></description></item><item><title>DataJoint</title><link>https://open-neuroscience.com/post/datajoint/</link><pubDate>Sun, 19 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/datajoint/</guid><description>&lt;p>DataJoint is an open-source library for managing and sharing scientific data pipelines in Python and Matlab.&lt;/p>
&lt;p>DataJoint allows creating and sharing computational data pipelines, which are defined as databases and analysis code for executing steps of activities for data collection and analysis. For example, many neuroscience studies are organized around DataJoint pipelines that start with basic information about the experiment, then ingest acquired data, and then perform processing, analysis, and visualization of results. The entire pipeline is diagrammed as a graph where each node is a table in the database with a corresponding class in the programming language; together they define the data structure and computations.&lt;/p>
&lt;p>DataJoint key features include:&lt;/p>
&lt;ul>
&lt;li>access to shared data pipelines in a relational database (MySQL-compatible) from Python, Matlab, or both.&lt;/li>
&lt;li>data integrity and consistency based founded on the relational data model and transactions&lt;/li>
&lt;li>an intuitive data definition language for pipeline design&lt;/li>
&lt;li>a diagramming notation to visualize data structure and dependencies&lt;/li>
&lt;li>a serialization framework: storing large numerical arrays and other scientific data in a language-independent way&lt;/li>
&lt;li>a flexible query language to retrieve precise cross-sections of data in a desired format&lt;/li>
&lt;li>automated execution of computational jobs, with built-in job management for distributed computing&lt;/li>
&lt;li>managed storage of large data objects outside the database&lt;/li>
&lt;/ul>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Dimitri Yatsenko; Edgar Walker; Fabian Sinz; Christopher Turner; Raphael Guzman&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://datajoint.io">https://datajoint.io&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Dimitri Yatsenko&lt;/p>
&lt;hr></description></item><item><title>cellfinder</title><link>https://open-neuroscience.com/post/cellfinder/</link><pubDate>Sun, 12 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/cellfinder/</guid><description>&lt;p>cellfinder is software from the Margrie Lab at the Sainsbury Wellcome Centre for automated 3D cell detection and registration of whole-brain images (e.g. serial two-photon or lightsheet imaging).&lt;/p>
&lt;p>It’s a work in progress, but cellfinder can:&lt;/p>
&lt;ul>
&lt;li>Detect labelled cells in 3D in whole-brain images (many hundreds of GB)&lt;/li>
&lt;li>Register the image to an atlas (such as the Allen Mouse Brain Atlas)&lt;/li>
&lt;li>Segment the brain based on the reference atlas&lt;/li>
&lt;li>Calculate the volume of each brain area, and the number of labelled cells within it&lt;/li>
&lt;li>Transform everything into standard space for analysis and visualisation&lt;/li>
&lt;/ul>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Adam Tyson&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/brainglobe/cellfinder">https://github.com/brainglobe/cellfinder&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Adam Tyson&lt;/p>
&lt;hr></description></item><item><title>DeepLabCut</title><link>https://open-neuroscience.com/post/deeplabcut/</link><pubDate>Sun, 12 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/deeplabcut/</guid><description>&lt;p>DeepLabCut™ is an efficient method for 3D markerless pose estimation based on transfer learning with deep neural networks that achieves excellent results (i.e. you can match human labeling accuracy) with minimal training data (typically 50-200 frames). We demonstrate the versatility of this framework by tracking various body parts in multiple species across a broad collection of behaviors.&lt;/p>
&lt;p>The package is open source, fast, robust, and can be used to compute 3D pose estimates. Please see the original paper and the latest work below. This package is collaboratively developed by the Mathis Group &amp;amp; Mathis Lab at EPFL/Harvard.&lt;/p>
&lt;p>The code is freely available and easy to install in a few clicks with Anaconda (and pypi). Please see instructions on deeplabcut.org. We also provide a very easy to use GUI interface, and a step-by-step user guide!&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Mackenzie Mathis, Alexander Mathis &amp;amp; contributors&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="http://deeplabcut.org/">http://deeplabcut.org/&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/channel/UC2HEbWpC_1v6i9RnDMy-dfA">https://www.youtube.com/channel/UC2HEbWpC_1v6i9RnDMy-dfA&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Mackenzie Mathis&lt;/p>
&lt;hr></description></item><item><title>Nilearn</title><link>https://open-neuroscience.com/post/nilearn/</link><pubDate>Sat, 11 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/nilearn/</guid><description>&lt;p>Nilearn is a Python module for fast and easy statistical learning on NeuroImaging data. It leverages the scikit-learn Python toolbox for multivariate statistics with applications such as predictive modelling, classification, decoding, or connectivity analysis.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>&lt;a href="https://github.com/orgs/nilearn/people">https://github.com/orgs/nilearn/people&lt;/a>&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="http://nilearn.github.io/">http://nilearn.github.io/&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Anonymous&lt;/p>
&lt;hr></description></item><item><title>DeepLabStream</title><link>https://open-neuroscience.com/post/deeplabstream/</link><pubDate>Sun, 05 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/deeplabstream/</guid><description>&lt;p>DeepLabStream is a python based multi-purpose tool that enables the realtime tracking of animals and manipulation of experiments. Our toolbox is adapted from the previously published DeepLabCut (Mathis et al., 2018) and expands on its core capabilities. DeepLabStreams core feature is the real-time analysis using any type of camera-based video stream (incl. multiple streams). Building onto that, we designed a full experimental closed-loop toolkit. It enables running experimental protocols that are dependent on a constant stream of bodypart positions and feedback activation of several input/output devices. It&amp;rsquo;s capabilities range from simple region of interest (ROI) based triggers to headdirection or behavior dependent stimulation.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Schwarz Neurocon Lab&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/SchwarzNeuroconLab/DeepLabStream">https://github.com/SchwarzNeuroconLab/DeepLabStream&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matias Andina&lt;/p>
&lt;hr></description></item><item><title>SpikeInterface</title><link>https://open-neuroscience.com/post/spikeinterface/</link><pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/spikeinterface/</guid><description>&lt;p>SpikeInterface is a unified Python framework for spike sorting. With its high-level API, it is designed to be accessible and easy to use, allowing users to build full analysis pipelines for spike sorting (reading-writing (IO) / preprocessing / spike sorting / postprocessing / validation / curation / comparison / visualization) with a few lines of code.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Alessio Buccino*; Cole Hurwitz*; Samuel Garcia; Jeremy Magland; Josh Siegle; Matthias Hennig&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/SpikeInterface/spikeinterface">https://github.com/SpikeInterface/spikeinterface&lt;/a>&lt;/p>
&lt;h2 id="project-video">Project Video&lt;/h2>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=nWJGwFB7oII">https://www.youtube.com/watch?v=nWJGwFB7oII&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Alessio Buccino&lt;/p>
&lt;hr></description></item><item><title>neuTube</title><link>https://open-neuroscience.com/post/neutube/</link><pubDate>Mon, 22 Jun 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/neutube/</guid><description>&lt;p>neuTube is an open source software for reconstructing neurons from fluorescence microscope images. It is easy to use and improves the efficiency of reconstructing neuron structures accurately. The framework combines 2D/3D visualization, semi-automated tracing algorithms, and flexible editing options that simplify the task of neuron reconstruction.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Ting Zhao&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://www.neutracing.com/">https://www.neutracing.com/&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Miguel Fernandes&lt;/p>
&lt;hr></description></item><item><title>Deep Cinac</title><link>https://open-neuroscience.com/post/deep_cinac/</link><pubDate>Sat, 06 Jun 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/deep_cinac/</guid><description>&lt;p>Two-photon calcium imaging is now widely used to infer neuronal dynamics from changes in fluorescence of an indicator. However, state of the art computational tools are not optimized for the reliable detection of fluorescence transients from highly synchronous neurons located in densely packed regions such as the CA1 pyramidal layer of the hippocampus during early postnatal stages of development. Indeed,the latest analytical tools often lack proper benchmark measurements. To meet this challenge, we first developed a graphical user interface allowing for a precise manual detection of all calcium transients from imaged neurons based on the visualization of the calcium imaging movie. Then, we analyzed the movies using a convolutional neural network with an attention process and a bidirectional long-short term memory network. This method is able to reach human performance and offers a better F1 score (harmonic mean of sensitivity and precision) than CaImAn to infer neural activity in the developingCA1 without any user intervention. It also enables automatically identifying activity originating from GABAergic neurons. Overall, DeepCINAC offers a simple, fast and flexible open-source toolbox for processing a wide variety of calcium imaging datasets while providing the tools to evaluate its performance.&lt;/p>
&lt;p>See full text at &lt;a href="https://www.biorxiv.org/content/10.1101/803726v2.full.pdf">https://www.biorxiv.org/content/10.1101/803726v2.full.pdf&lt;/a>&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Julien Denis&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://gitlab.com/cossartlab/deepcinac">https://gitlab.com/cossartlab/deepcinac&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matias Andina&lt;/p>
&lt;hr></description></item><item><title>VocalMat</title><link>https://open-neuroscience.com/post/vocalmat/</link><pubDate>Wed, 27 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/vocalmat/</guid><description>&lt;p>Mice emit ultrasonic vocalizations (USV) to transmit socially-relevant information. To detect and classify these USVs, here we describe the development of VocalMat. VocalMat is a software that uses image-processing and differential geometry approaches to detect USVs in audio files, eliminating the need for user-defined parameter tuning. VocalMat also uses computational vision and machine learning methods to classify USVs into distinct categories. In a dataset of &amp;gt;4,000 USVs emitted by mice, VocalMat detected more than &amp;gt;98% of the USVs and accurately classified ≈86% of USVs when considering the most likely label out of 11 different USV types. We then used Diffusion Maps and Manifold Alignment to analyze the probability distribution of USV classification among different experimental groups, providing a robust method to quantify and qualify the vocal repertoire of mice. Thus, VocalMat allows accurate and highly quantitative analysis of USVs, opening the opportunity for detailed and high-throughput analysis of this behavior.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Antonio H. O. Fonseca, Gustavo M. Santana, Sergio Bampi, Marcelo O Dietrich&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://www.dietrich-lab.org/vocalmat">https://www.dietrich-lab.org/vocalmat&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Matias Andina&lt;/p>
&lt;hr></description></item><item><title>3D Slicer</title><link>https://open-neuroscience.com/post/3d_slicer/</link><pubDate>Fri, 22 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/3d_slicer/</guid><description>&lt;p>3D Slicer is a software for medical image informatics, image processing, and three-dimensional visualization. It’s extremely powerful and versatile with plenty of different options. It is a great tool for volume rendering, registration, interactive segmentation of images and even offers the possibility of running Python scripts thought an embedded Python interpreter.&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Ron Kikinis&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/Slicer/Slicer">https://github.com/Slicer/Slicer&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Miguel Fernandes&lt;/p>
&lt;hr></description></item><item><title>Colaboratory</title><link>https://open-neuroscience.com/post/colaboratory/</link><pubDate>Sun, 10 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/colaboratory/</guid><description>&lt;p>Colaboratory is a free Jupyter notebook environment that runs in the cloud. Your notebooks get stored on Google Drive. The great advantage is that you don’t have to install anything (however, for some features you need a Google account) on your system to use it. You can perform specific computations during data analysis with pre-installed Python libraries and gives you access to accelerated hardware for free (e.g. GPUs and TPUs).&lt;/p>
&lt;h2 id="project-authors">Project Author(s)&lt;/h2>
&lt;p>Google&lt;/p>
&lt;h2 id="project-links">Project Links&lt;/h2>
&lt;p>&lt;a href="https://colab.research.google.com/notebooks/intro.ipynb">https://colab.research.google.com/notebooks/intro.ipynb&lt;/a>&lt;/p>
&lt;hr>
&lt;p>This post was automatically generated by
Miguel Fernandes&lt;/p>
&lt;hr></description></item><item><title>NiBabel</title><link>https://open-neuroscience.com/post/nibabel/</link><pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/nibabel/</guid><description>&lt;p>&lt;a href="http://nipy.org/nibabel/" target="_blank" rel="noopener">NiBabel&lt;/a> is a python package, under the NiPy project, that aims at unifying the process of opening different medical and neuroimaging file formats, including: &lt;a href="http://www.grahamwideman.com/gw/brain/analyze/formatdoc.htm" target="_blank" rel="noopener">ANALYZE&lt;/a>,&lt;a href="http://www.nitrc.org/projects/gifti" target="_blank" rel="noopener">GIFTI&lt;/a>, &lt;a href="http://nifti.nimh.nih.gov/nifti-1/" target="_blank" rel="noopener">NIfTI1&lt;/a>, &lt;a href="http://en.wikibooks.org/wiki/MINC/Reference/MINC2.0_File_Format_Reference" target="_blank" rel="noopener">MINC&lt;/a>, &lt;a href="http://surfer.nmr.mgh.harvard.edu/fswiki/FsTutorial/MghFormat" target="_blank" rel="noopener">MGH&lt;/a> and &lt;a href="http://xmedcon.sourceforge.net/Docs/Ecat" target="_blank" rel="noopener">ECAT&lt;/a> as well as PAR/REC. The package is also able to read and write &lt;a href="http://surfer.nmr.mgh.harvard.edu/" target="_blank" rel="noopener">Freesurfer&lt;/a> format.&lt;/p>
&lt;div align="center">
&lt;p>&lt;img src="https://nipy.org/nibabel/_static/nipy-logo-bg-138x120.png" alt="">&lt;/p>
&lt;/div></description></item><item><title>Nipy</title><link>https://open-neuroscience.com/post/nipy/</link><pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/nipy/</guid><description>&lt;p>&lt;a href="https://nipy.org/" target="_blank" rel="noopener">NiPy&lt;/a> is an effort to make brain imaging research easier and more clear. This is implemented by providing a series of software that deal with file IO, analysis, and interfaces &amp;amp; pipelines.&lt;/p>
&lt;p>The software present up to now (05/05/2020)&lt;/p>
&lt;p>&lt;a href="https://nipy.org/packages/nipype/index.html" target="_blank" rel="noopener">nipype&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://nipy.org/packages/dipy/index.html" target="_blank" rel="noopener">diPy&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://nipy.org/packages/mindboggle/index.html" target="_blank" rel="noopener">mindboggle&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://nipy.org/packages/nibabel/index.html" target="_blank" rel="noopener">NiBabel&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://nipy.org/packages/sdm/index.html" target="_blank" rel="noopener">scitran SDM&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://nipy.org/packages/nipy/index.html" target="_blank" rel="noopener">nipy&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://nipy.org/packages/nitime/index.html" target="_blank" rel="noopener">nitime&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://nipy.org/packages/popeye/index.html" target="_blank" rel="noopener">popeye&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://nipy.org/packages/nilearn/index.html" target="_blank" rel="noopener">niLearn&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://nipy.org/packages/pymvpa/index.html" target="_blank" rel="noopener">PyMVPA&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://nipy.org/packages/mne/index.html" target="_blank" rel="noopener">MNE&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://nipy.org/packages/niwidgets/index.html" target="_blank" rel="noopener">niwidgets&lt;/a>&lt;/p></description></item><item><title>BrainBrowser</title><link>https://open-neuroscience.com/post/brainbrowser/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/brainbrowser/</guid><description>&lt;p>BrainBrowser is a collection of open source, web-based 3D data visualization tools, mainly for neuroimaging studies. It is built using open technologies such as WebGL and HTML5. It allows exploration of cortical surface models (MNI and Wavefront OBJ, as well as FreeSurfer ASCII surface format) and volumetric MINC data. This project is currently maintained by &lt;a href="http://www.tareksherif.ca/" target="_blank" rel="noopener">Tarek Sherif&lt;/a> at McGill University, and the source code is available on &lt;a href="https://github.com/aces/brainbrowser" target="_blank" rel="noopener">GitHub&lt;/a>.&lt;/p>
&lt;p>You can find more info on the &lt;a href="https://brainbrowser.cbrain.mcgill.ca/" target="_blank" rel="noopener">website&lt;/a>&lt;/p></description></item><item><title>Fiji</title><link>https://open-neuroscience.com/post/fiji/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/fiji/</guid><description>&lt;p>&lt;a href="http://fiji.sc/Fiji" target="_blank" rel="noopener">Fiji&lt;/a> is a distribution of ImageJ. The idea of the developers is to make the life of scientists easier by bundling ImageJ with nicely organised plugins and auto update function.&lt;/p>
&lt;blockquote>
&lt;p>Fiji compares to ImageJ as Ubuntu compares to Linux.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a href="http://openeuroscience.wordpress.com/software/imagej/" title="ImageJ" target="_blank" rel="noopener"> &lt;img src="https://i1.wp.com/rsbweb.nih.gov/ij/images/imagej-logo.gif?w=800" alt="imagej logo" data-recalc-dims="1" />&lt;/a>&lt;/p></description></item><item><title>PySpace</title><link>https://open-neuroscience.com/post/pyspace/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/pyspace/</guid><description>&lt;p>&lt;a href="https://pyspace.github.io/pyspace/" target="_blank" rel="noopener">PySpace&lt;/a> is a signal processing and classificiation environment for Python.&lt;/p>
&lt;p>Modular software for processing of large data streams that has been specifically designed to enable distributed execution and empirical evaluation of signal processing chains. Various signal processing algorithms are available within the software, from finite impulse response filters over data-dependent spatial filters (e.g. CSP, xDAWN) to established classifiers (e.g. SVM, LDA). pySPACE incorporates the concept of node and node chains of the Modular Toolkit for Data Processing (MDP) framework.&lt;/p>
&lt;p>A paper about PySpace can be found &lt;a href="http://journal.frontiersin.org/article/10.3389/fninf.2013.00040/full" target="_blank" rel="noopener">here&lt;/a>&lt;/p></description></item><item><title>Tensor Flow</title><link>https://open-neuroscience.com/post/tensor-flow/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://open-neuroscience.com/post/tensor-flow/</guid><description>&lt;p>Google has packaged their deeplearning machine learning tools and made it open source. The project is called tensorflow, and is available &lt;a href="http://www.tensorflow.org" target="_blank" rel="noopener">here.&lt;/a> Some nice tutorials on the website, so that with a bit of patience, people can start to deep their toes into machine learning!&lt;/p>
&lt;p>Be sure to check the video below for more details!&lt;/p>
&lt;p>&lt;span class="embed-youtube" style="text-align:center; display: block;">&lt;/span>&lt;/p></description></item></channel></rss>